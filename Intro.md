# Large Language Model
- ability to achieve general purpose language generation and understanding
- acquire these abilities by learning statistical relationships from text documents during a computationally intensive self supervised or semi supervised traning process
- they are artificial neural networks, the largest and the most capable ones are built on transformer based architecture
- Some recent implementations are based on other architectures such as recurrent neural networks variants and Mamba
- Can be used for text generation, a form of Generative AI by taking an input text and repeatedly predicting the next token or work
